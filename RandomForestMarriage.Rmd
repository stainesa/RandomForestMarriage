---
title: "Random Forest"
author: "Anthony Staines"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  word_document: default
  html_document: default
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
rm(list=ls())

library(tidyverse)
library(randomForest)
library(ranger)
library(rsample)
library(rpart)
library(tidymodels)
library(ggplot2)
library(dendextend)
library(ggdendro)
library(plotROC)

knitr::opts_chunk$set(echo = FALSE, cache = TRUE)
set.seed(412789)
```

# Introduction - why examine Random Forest models in particular?

There are numerous models used in machine learning, some complex beyond any explanation we can provide - for example the large language models at the foundation of many commercial AI projects, butsome simpler, and more open to explanation,

To examine the behaviour, and some of the limitations in real use, of machine learning, these simpler models are preferable. In a real modelling context several different families of model might be chosen, developed and applied to a particular task. Random forest models, which are the focus of this paper, are popular because they are flexible, usually perform well, and are not too difficult to use.

To understand random forest models, it is first necessary to understand Classification and Regression Trees (CART), which are both a modelling approach in their own right, and the constituents of the forest. These are models designed to develop a classification, or a regression, based on training data.

In this context a classification is a decision as to which one of a number of groups any particular item falls into. In the substantive example, the question is, based on data recorded before marriage, should a proposed marriage be further investigated as a potential sham marriage or not. There are two possible outcomes, yes and no. In other situations, there may be several possible outcomes, or it may be desired to predict a value for an outcome, where the outcome is a number, in which case the analysis is a form of regression.

## Data used

The data used by the UK Home Office to develop their model are not available, and indeed, the majority of the variables included in the final model are not identified. For purpose of illustration, a commonly used, and publicly available data set is desirable, as a substitute.

We use, for demonstration purposes, the 'Attrition' data set, which is employee attrition information originally provided by the IBM Watson Analytics Lab. These data were collected to model employee leaving - hence the term attrition. 

The original website link, (https://www.kaggle.com/datasets/pavansubhasht/ibm-hr-analytics-attrition-dataset/data), describes the data thus - 
“Uncover the factors that lead to employee attrition and explore important questions such as ‘show me a breakdown of distance from home by job role and attrition’ or ‘compare average monthly income by education and attrition’. This is a fictional data set created by IBM data scientists.”

Our first task is to briefly survey these data, and see what variables are present, and what set of values they take.

```{r load attrition}
data("attrition", package = "modeldata")
class(attrition)
Attrition <- attrition
rm(attrition)
```

```{r skimr}
skimr::skim(Attrition)
```

We also look specifically at the outcome - Attrition from employment.
 
```{r attrition table}
table(Attrition$Attrition)
```

Most people stayed in work, but just under 1 in 5 left.

```{r Recode Job role and StockOption}
Attrition <- Attrition %>%
    mutate(JobRole = case_match(JobRole,
        c('Laboratory_Technician', 'Sales_Executive', 'Sales_Representative') ~ 'Sales/Tech',
        c('Healthcare_Representative', 'Human_Resources', 'Manager', 'Manufacturing_Director', 'Research_Director', 'Research_Scientist') ~ 'Research/Manager/Rep')) %>%
    mutate(StockOptionLevel = case_match(StockOptionLevel,
                                    0 ~ 'No',
                                    1 ~ 'Yes')) %>%
    mutate(Income = case_when(
        MonthlyIncome >= 2475 ~ 0,
        MonthlyIncome < 2475 ~ 1))

```

Given these data, the aim of a CART is to fit a model that best predicts the observed attrition from the data recorded. There are quite a few tools for this, and we use the rpart package from R. This starts with every variable in the data set, and produces the results shown.

```{r rpart Short Tree}
Tree <- rpart(Attrition ~., data=Attrition)
Tree_0.03 <- rpart::prune(Tree, cp = 0.03)
Tree_0.02 <- rpart::prune(Tree, cp = 0.02)
```

```{r Plot of simpler tree 0.03}
rpart.plot::rpart.plot(Tree_0.03,
 type = 3, clip.right.labs = FALSE, branch = .3, under = TRUE)

```

This is a single CART (a tree), and it is not too bad at predicting attrition.It is read from top to bottom. 

* The first split, is at OverTime, and it seems that staff who are paid for overtime are more likely to move on (Attrition), than those who are not. Payment for over time reflects job grade.

* The second split here is amongst those paid for over time work, into those with higher and lower salaries, where those with lower salaries are more mobile than those with higher salaries.The cut off at $2475 is about the 15th centile of monthly income.

* The leaves, the bits at the end drawn as circles, are labelled with the most likely outcomes on that branch (Yes or No to leaving), the proportion leaving, and the percentage of the workforce on that branch.

This is one tree.

Here is another one. This comes from a trivial adjustment of one parameter in the model, a parameter (a lever if you like), which allows the software to choose deeper and more complicated trees.

Although the two trees are similar, in that both start with the same two variables, paid overtime, and salary level, they are not the same.

```{r Plot of more complex tree 0.02}
rpart.plot::rpart.plot(Tree_0.02,
 type = 3, clip.right.labs = FALSE, branch = .3, under = TRUE)

```

The point here is that a very minor choice of parameter, can produce a very different tree. Which of these is correct is probably not the right question, as both are justifiable, but one may be more useful than the other for a particular purpose.

Theses two trees have been pruned - this means that splits which are numerically less important have been removed. The full tree looks like this.

```{r Full tree graph}
rpart.plot::rpart.plot(Tree,
 type = 3, clip.right.labs = FALSE, branch = .3, under = TRUE)

```

```{r Prediction table}
table(predict(Tree, type = "class"), Attrition$Attrition)
table(predict(Tree_0.03, type = "class"), Attrition$Attrition)
table(predict(Tree_0.02, type = "class"), Attrition$Attrition)

```

There are several different ways to examine the performance of a classification model. Which one to use, depends on the goals of the modelling exercise. A popular choice is to use a measure called the 'Area Under the Curve (AUC)'. This is useful when there is no difference between the value of falsely identifying a person, as a member of a class, (a false positive), and falsely rejecting a person, who is actually a member of a class (a false negative). This value is compared with 0.5, which is the result of a coin toss for two classes, and 1.0, which is a perfect classification, which makes no mistakes.

This graph shows the origin of the concept. The thin diagonal line is the performance of a random classifier, equivalent to tossing a coin to classify each person. The gap between the thicker line, and the diagonal is how much better the CART tree does at classification than random chance. The area under the curve is a number measuring this performance.

```{r Performance of CART model}
Tree_Prediction <- ROCR::prediction(
    predict(Tree, type = "prob")[,2],
    Attrition$Attrition)
Tree_Prediction_0.02 <- ROCR::prediction(
    predict(Tree_0.02, type = "prob")[,2],
    Attrition$Attrition)
Tree_Prediction_0.03 <- ROCR::prediction(
    predict(Tree_0.03, type = "prob")[,2],
    Attrition$Attrition)

    DATA <- Attrition %>%
        select(Attrition) %>%
        mutate(Attrition = case_match(Attrition,
                                      'Yes' ~ 1,
                                      'No' ~ 0)) %>%
        bind_cols(Prediction =
            unlist(Tree_Prediction@predictions,
                use.names = FALSE)) %>%
        bind_cols(Prediction_0.02 =
            unlist(Tree_Prediction_0.02@predictions,
                use.names = FALSE)) %>%
        bind_cols(Prediction_0.03 =
            unlist(Tree_Prediction_0.03@predictions,
                use.names = FALSE))
```

```{r Prepare graphics}
gTree <- ggplot(DATA,aes(d = Attrition,
                         m = Prediction)) +
    geom_roc(colour = 'blue', labelsize = 3) +
    geom_abline(color='red', slope = 1, intercept = 0) 

gTree_0.02 <- ggplot(DATA,aes(d = Attrition,
                         m = Prediction_0.02))  +
    geom_roc(colour = 'blue', labelsize = 3) +
    geom_abline(color='red', slope = 1, intercept = 0)

gTree_0.03 <- ggplot(DATA,aes(d = Attrition,
                         m = Prediction_0.03)) +
    geom_roc(colour = 'blue', labelsize = 3) +
    geom_abline(color='red', slope = 1, intercept = 0)

AUC <- calc_auc(gTree)
AUC_0.02 <- calc_auc(gTree_0.02)
AUC_0.03 <- calc_auc(gTree_0.03)
```


```{r Draw graphs}
gTree +
    labs(title = 'Full tree',
         subtitle = paste('Area under curve = ', round(AUC$AUC, 2)),
         x = "False positives",
         y = "True positives",
    )

gTree_0.02 +
    labs(title = 'Pruned tree 1',
         subtitle = paste('Area under curve = ', round(AUC_0.02$AUC, 2)),
         x = "False positives",
         y = "True positives",
    )

gTree_0.03 +
    labs(title = 'Pruned tree 2',
         subtitle = paste('Area under curve = ', round(AUC_0.03$AUC, 2)),
         x = "False positives",
         y = "True positives",
    )
```


# From a tree to a forest

While CART is a useful technique, it has some significant weaknesses. In particular it is very dependent on the exact data fed to it, and often works very poorly on new data. Given that the purpose of the Home Office work is to identify future marriages that are felt to be at high risk of being sham marriages, this is not desirable.

The key idea behind random forests is to look across many possible classification trees, rather than looking for a single optimum tree. These trees are designed to be different, and to cover a very wide range of possible variables. The final classification is made across all of these trees. A more detailed non-technical introduction is here, and the original work is from Leo Breiman.

How this works bears further consideration. Models are developed on a set of data, in our example case, fictitious data based on people leaving IBM, and in our substantive case, data collected from UK marriage registrars, at some time before a proposed marriage.



In our example, we start by dividing the data into two sets, half a training dataset, and half a test dataset.

```{r}
# number of features
n_features <- length(setdiff(names(Attrition), "Attrition"))

# train a default random forest model
attr_rf1 <- ranger(
  Attrition ~ ., 
  data = Attrition,
  mtry = floor(n_features / 3),
  importance = 'impurity',
  respect.unordered.factors = "order",
  seed = 123
)

# get OOB RMSE
(default_rmse <- sqrt(attr_rf1$prediction.error))
## [1] 24859.27

treeInfo(attr_rf1)#  
importance(attr_rf1)


```

# Variable importance measure

```{r}

p1 <- vip::vip(attr_rf1, num_features = 25, bar = FALSE)
p1
vip::vip(gTree)
```
