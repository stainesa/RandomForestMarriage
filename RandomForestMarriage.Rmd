---
title: "Random Forest"
author: "Anthony Staines"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  word_document: default
  html_document: default
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
rm(list=ls())

library(tidyverse)
library(randomForest)
library(ranger)
library(rsample)
library(rpart)
library(tidymodels)
library(ggplot2)
library(dendextend)
library(ggdendro)
library(plotROC)
library(knitr)
#library(kable)
library(kableExtra)
library(vip)


knitr::opts_chunk$set(echo = FALSE, cache = TRUE)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)

set.seed(979)

```

# Introduction - why examine Random Forest models in particular?

There are numerous models used in machine learning, some complex beyond any explanation we can provide - for example the large language models at the foundation of many commercial AI projects, but some are simpler, and more open to explanation. To examine the behaviour, and some of the limitations in real use, of machine learning, these simpler models are preferable.

In a real modelling context several different families of model might be chosen, developed and applied to a particular task. There is an extensive literature on how to do this. For the purposes of this work one single model family is chosen and simple models fitted, to illustrate some of the strengths and weaknesses of these approaches.

Random forest models, which are the focus of this paper, are popular because they are flexible, usually perform well, and are not too difficult to use. To understand random forest models, it is first necessary to understand Classification and Regression Trees (CART), which are both a modelling approach in their own right, and the constituents of the forest. These are models designed to develop a classification, or a regression, based on training data.

In this context a classification is a decision as to which one of a number of groups any particular item falls into. In the substantive example, the question is, based on data recorded before marriage, should a proposed marriage be further investigated as a potential sham marriage or not. There are two possible outcomes, 'yes' and 'no'. In other situations, there may be several possible outcomes, or it may be desired to predict a value for an outcome, where the outcome is a number, in which case the analysis is some form of regression.

## Data used

The data used by the UK Home Office to develop their model are not available, and indeed, the majority of the variables included in the final model are not identified. For purpose of illustration, a commonly used, and publicly available data set is desirable, as a substitute. We use, for demonstration purposes only, the 'Attrition' data set, which is employee attrition information originally provided by the IBM Watson Analytics Lab. These data were collected to model employee leaving - hence the term 'attrition'.

The original website link, which is accessible through the Kaggle machine learning website, (https://www.kaggle.com/datasets/pavansubhasht/ibm-hr-analytics-attrition-dataset/data), describes the data thus - 
“Uncover the factors that lead to employee attrition and explore important questions such as ‘show me a breakdown of distance from home by job role and attrition’ or ‘compare average monthly income by education and attrition’. This is a *fictional* data set created by IBM data scientists.”

What data are contained in this set? Our first task is to briefly survey these data, and see what variables are present, and what set of values they take.

```{r load attrition}
data("attrition", package = "modeldata")
class(attrition)
Attrition <- attrition
rm(attrition)
```

```{r Table1, results='asis', tab.cap = 'Description of variables in the data set, for those who left, and stayed in IBM'}
TABLE1 <- table1::table1(~. | Attrition,
               data = Attrition %>%
                 mutate(Attrition =
                          case_match(Attrition,
                                     'Yes' ~ 'Left',
                                     'No' ~ 'Stayed')),
               overall = FALSE)

kbl(TABLE1, longtable = T, booktabs = T, caption = "Longtable") %>%
  kable_styling(latex_options = c("repeat_header"))

```

We also look specifically at the outcome - Attrition from employment.

```{r Attrition table}
table(Attrition$Attrition)
```

Most people stayed in work, but just under 1 in 5 left. Note that this represents the actual attrition - these are staff who left employment. Were Attrition to be interpreted as being a measure of the risk for leaving, model interpretation would become more challenging, and this will be discussed further below.

There are almost 1,500 people in these data, and these can conveniently be divided into a training set, which is used to develop a model, and a test set, which is used to see how well the model performs on new data. In most applications of machine learning, the latter is the key question. We divide the data with 3 in 4 going to the training set, and 1 in 4 going to the test set, while ensuring that this selection is done within the two values of Attrition, so that there are the same proportion of leavers and non-leavers in both groups.

```{r Recode}
# First we recode some variables to more useful values, and add  a third variable.
Attrition <- Attrition %>%
    mutate(JobRole = case_match(JobRole,
                                c('Laboratory_Technician', 'Sales_Executive', 'Sales_Representative') ~ 'Sales/Tech',
                                c('Healthcare_Representative', 'Human_Resources', 'Manager', 'Manufacturing_Director', 'Research_Director', 'Research_Scientist') ~ 'Research/Manager/Rep')) %>%
    mutate(EducationField = case_match(EducationField,
                                       c('Human_Resources','Marketing', 'Other') ~ 'Non-technical',
                                       c('Life_Sciences','Medical','Technical_Degree') ~ 'Technical')) %>%
    mutate(BusinessTravel = case_match(BusinessTravel,
                                       c('Non-Travel') ~ 'None',
                                       c('Travel_Rarely') ~ 'Seldom',
                                       c('Travel_Frequently') ~ 'Often')) 

```

```{r split into test and training data}

#Then we split
Attrition_split <- initial_split(Attrition, prop = 3/4, strata = Attrition)

# Add a training variable to Attrition

Attrition <- Attrition %>%
  mutate(Training = FALSE)

# Add a variable to indicate training set or test set.
 Attrition$Training[Attrition_split$in_id] <- TRUE
    table(Attrition$Attrition,Attrition$Training)
    
    
  TEST <- Attrition %>% filter(!Training) %>% select(-Training)
#  TEST  <-  testing(Attrition_split)
  TRAIN <- Attrition %>% filter(Training) %>% select(-Training)
#  TRAIN <- training(Attrition_split)
```

Given these data, the aim of a CART is to fit a model that best predicts the observed attrition from the data recorded. There are quite a few tools for this, and we use the rpart package from R. This starts with every variable in the data set, and produces a tree, as shown.

```{r rpart Create three trees}
Tree_0.01 <- rpart::rpart(Attrition ~., data = training(Attrition_split))
Tree_0.02 <- rpart::prune(Tree_0.01, cp = 0.02)
Tree_0.03 <- rpart::prune(Tree_0.01, cp = 0.03)
```

The first tree to emerge from this process is rather complicated, and perhaps too complicated to be of value. Such complex trees tend to overfit the data they are given, and perform markedly worse on new data than simpler trees.

```{r Tree_0.01 plot}
rpart.plot::rpart.plot(Tree_0.01,
                       type = 3, clip.right.labs = FALSE, branch = .3, under = TRUE, cex=0.8)

```

It is possible to derive measure of the relative impact on the classification of the variables included in a tree, and this is referred to as 'variable importance'. For this tree the variable importance is shown in the table.

```{r complex tree importance plot}

ggplot(data = vip::vi(Tree_0.01) %>% arrange(Importance) %>% filter(Importance > 5),
       aes(y = forcats::fct_inorder(Variable), x = Importance)) +
    labs(title = 'Variable importance From a Classification Tree',
         subtitle = 'Importance over 5',
         x  = 'Variable importance (model based measure)',
         y = 'Variable') +
    geom_col(colour = 'black', fill = 'lightblue') +
    theme_minimal() +
    theme(text = element_text(size = 25))
```

It's possible, and desirable, to simplify these trees, and a complexity penalty can be provided in the model for overly complex trees. This is set at 0.01, by default, but choosing to increase it to 0.03, a modest change, produces a very different, and perhaps more comprehensible, picture. This is referred to as 'pruning'.

```{r Tree_0.03}
rpart.plot::rpart.plot(Tree_0.03,
 type = 3, clip.right.labs = FALSE, branch = .3, under = TRUE)

```

This is a single CART (a tree), and it performs reasonably for predicting attrition. It is read from top to bottom. 

* The first split, is at OverTime, and it seems that staff who are paid for overtime are more likely to move on (Attrition), than those who are not. Payment for overtime reflects job grade.

* The second split here is amongst those paid for overtime work, into those with higher and lower salaries, where those with lower salaries are more mobile than those with higher salaries.The cut off at $2475 is about the 15th centile of monthly income.

* The leaves, the pieces at the end drawn as circles, are labelled with the most likely outcomes on that branch (Yes or No to leaving), the proportion leaving, and the percentage of the workforce on that branch.

These splits can be chosen in a number of different ways in different CART packages. In this case, using the rpart package, splits are chosen to maximise the uniformity of the groups after each split.

Setting the complexity penalty to 0.02 we produce another tree. Although the two pruned trees are similar, in that both start with the same two variables, paid overtime, and salary level, they are not the same.

```{r Plot of Tree_0.02}
rpartPlot_0.02 <- rpart.plot::rpart.plot(Tree_0.02,
 type = 3, clip.right.labs = FALSE, branch = .3, under = TRUE)

```

The point is that a very minor change in one parameter, can produce a very different tree. Which of these is correct is probably not the right question, as both of the simpler trees are justifiable, but one may be more useful than the other for a particular purpose.

## Performace of CAR Tree models

There are several different ways to examine the performance of a classification model. Which one to use, depends mainly on the goals of the modelling exercise, and specifically on the costs of misclassification. One popular choice is a measure called the 'Area Under the Curve (AUC)'. This is useful when there is no difference between the cost of falsely identifying a person, as a member of a class, (a false positive), and falsely rejecting a person, who is actually a member of a class (a false negative). This value is compared with 0.5, which is the result of a coin toss, that is a random classification, for two classes, and 1.0, which is a perfect classification, which makes no mistakes.

These graphs, which are known as Receiver Operating Characteristics (ROC) curves (Peterson et al. 1954), show the meaning of the term 'Area under the curve'. The thin diagonal line is the performance of a random classifier, equivalent to tossing a coin to classify each person. The gap between the thicker line, and the diagonal is how much better the CART tree does at classification than random chance. The area under the curve is a number measuring this performance.

```{r Performance of CART model - TRAINING}
Tree_Prediction_0.01a <- ROCR::prediction(
    predict(Tree_0.01, TRAIN, type = "prob")[,2],
    TRAIN$Attrition)
Tree_Prediction_0.02a <- ROCR::prediction(
    predict(Tree_0.02, TRAIN, type = "prob")[,2],
    TRAIN$Attrition)
Tree_Prediction_0.03a <- ROCR::prediction(
    predict(Tree_0.03, TRAIN, type = "prob")[,2],
    TRAIN$Attrition)

#training(Attrition_split) %>%
    DATA_train <- TRAIN %>%
        select(Attrition) %>%
        mutate(Attrition = case_match(Attrition,
                                      'Yes' ~ 1,
                                      'No' ~ 0)) %>%
        bind_cols(Prediction =
            unlist(Tree_Prediction_0.01a@predictions,
                use.names = FALSE)) %>%
        bind_cols(Prediction_0.02 =
            unlist(Tree_Prediction_0.02a@predictions,
                use.names = FALSE)) %>%
        bind_cols(Prediction_0.03 =
            unlist(Tree_Prediction_0.03a@predictions,
                use.names = FALSE))
```

```{r Performance of CART model - TEST}
Tree_Prediction_0.01b <- ROCR::prediction(
    predict(Tree_0.01, TEST, type = "prob")[,2],
    TEST$Attrition)
Tree_Prediction_0.02b <- ROCR::prediction(
    predict(Tree_0.02, TEST, type = "prob")[,2],
    TEST$Attrition)
Tree_Prediction_0.03b <- ROCR::prediction(
    predict(Tree_0.03, TEST, type = "prob")[,2],
    TEST$Attrition)

#testing(Attrition_split) %>%
    DATA_test <- TEST %>% 
        select(Attrition) %>%
        mutate(Attrition = case_match(Attrition,
                                      'Yes' ~ 1,
                                      'No' ~ 0)) %>%
        bind_cols(Prediction =
            unlist(Tree_Prediction_0.01b@predictions,
                use.names = FALSE)) %>%
        bind_cols(Prediction_0.02 =
            unlist(Tree_Prediction_0.02b@predictions,
                use.names = FALSE)) %>%
        bind_cols(Prediction_0.03 =
            unlist(Tree_Prediction_0.03b@predictions,
                use.names = FALSE))
```

```{r Prepare graphics - TRAIN}
gTree_0.01_train <- ggplot(DATA_train, aes(d = Attrition,
                         m = Prediction)) +
    geom_roc(colour = 'blue', labelsize = 3) +
    geom_abline(color='red', slope = 1, intercept = 0) 

gTree_0.02_train <- ggplot(DATA_train, aes(d = Attrition,
                         m = Prediction_0.02))  +
    geom_roc(colour = 'blue', labelsize = 3) +
    geom_abline(color='red', slope = 1, intercept = 0)

gTree_0.03_train <- ggplot(DATA_train, aes(d = Attrition,
                         m = Prediction_0.03)) +
    geom_roc(colour = 'blue', labelsize = 3) +
    geom_abline(color='red', slope = 1, intercept = 0)

AUC_0.01_train <- calc_auc(gTree_0.01_train)
AUC_0.02_train <- calc_auc(gTree_0.02_train)
AUC_0.03_train <- calc_auc(gTree_0.03_train)
```

```{r Prepare graphics - TEST}
gTree_0.01_test <- ggplot(DATA_test, aes(d = Attrition,
                         m = Prediction)) +
    geom_roc(colour = 'blue', labelsize = 3) +
    geom_abline(color='red', slope = 1, intercept = 0) 

gTree_0.02_test <- ggplot(DATA_test, aes(d = Attrition,
                         m = Prediction_0.02))  +
    geom_roc(colour = 'blue', labelsize = 3) +
    geom_abline(color='red', slope = 1, intercept = 0)

gTree_0.03_test <- ggplot(DATA_test, aes(d = Attrition,
                         m = Prediction_0.03)) +
    geom_roc(colour = 'blue', labelsize = 3) +
    geom_abline(color='red', slope = 1, intercept = 0)

AUC_0.01_test <- calc_auc(gTree_0.01_test)
AUC_0.02_test <- calc_auc(gTree_0.02_test)
AUC_0.03_test <- calc_auc(gTree_0.03_test)
```

```{r Draw graphs}
gTree_0.01_train <- gTree_0.01_train +
    labs(title = 'Full tree - Training data',
         subtitle = paste('Area under curve = ', round(AUC_0.01_train$AUC, 2)),
         x = "False positives",
         y = "True positives",
    )

gTree_0.02_train <- gTree_0.02_train +
    labs(title = 'Pruned tree 1  - Training data',
         subtitle = paste('Area under curve = ', round(AUC_0.02_train$AUC, 2)),
         x = "False positives",
         y = "True positives",
    )

gTree_0.03_train <- gTree_0.03_train +
    labs(title = 'Pruned tree 2 - Training data',
         subtitle = paste('Area under curve = ', round(AUC_0.03_train$AUC, 2)),
         x = "False positives",
         y = "True positives",
    )


gTree_0.01_test <- gTree_0.01_test +
    labs(title = 'Full tree - Test data',
         subtitle = paste('Area under curve = ', round(AUC_0.01_test$AUC, 2)),
         x = "False positives",
         y = "True positives",
    )

gTree_0.02_test <- gTree_0.02_test +
    labs(title = 'Pruned tree 1  - Test data',
         subtitle = paste('Area under curve = ', round(AUC_0.02_test$AUC, 2)),
         x = "False positives",
         y = "True positives",
    )

gTree_0.03_test <- gTree_0.03_test +
    labs(title = 'Pruned tree 2 - Test data',
         subtitle = paste('Area under curve = ', round(AUC_0.03_test$AUC, 2)),
         x = "False positives",
         y = "True positives",
    )

cowplot::plot_grid(gTree_0.01_train, gTree_0.01_test, gTree_0.02_train,
                   gTree_0.02_test, gTree_0.03_train, gTree_0.03_test,
                   ncol = 2, labels=c())
```


```{r Tidy up CART}
rm(Tree_0.01, Tree_0.02, Tree_0.03)
rm(Tree_Prediction_0.01a, Tree_Prediction_0.02a, Tree_Prediction_0.03a)
rm(Tree_Prediction_0.01b, Tree_Prediction_0.02b, Tree_Prediction_0.03b)
rm(gTree_0.01_test, gTree_0.01_train, gTree_0.02_test,
   gTree_0.02_train, gTree_0.03_test, gTree_0.03_train)
rm(AUC_0.01_test, AUC_0.01_train, AUC_0.02_test,
   AUC_0.02_train, AUC_0.03_test, AUC_0.03_train)
rm(DATA_test, DATA_train)
rm(rpartPlot_0.02, TABLE1)
```

While the full model does much better on the training data than the simpler models, on the test data its performance is no better, and none of these classifiers would be regarded as very effective in practice. There are a number of ways to improve the performance of these trees, notably cross-validation, and a range of methods for improving balance. Any introduction to machine learning would cover these extensively, for example .... However, for this analysis, following these important ideas further would distract from the focus on the social implications of the use of the Home Office model. 

# From a tree to a forest

While CART is a useful technique, it has some significant weaknesses. In particular it is very dependent on the exact data fed to it, and often works very poorly on new data. Given that the purpose of the Home Office work is to identify future marriages that are felt to be at high risk of being sham marriages, this is not desirable. For this reason, it is common to use an extension of CART, known as random forests.

The key idea behind random forests is to look across many possible classification trees, rather than looking for a single optimum tree. These trees are designed to be different, and to cover a very wide range of possible variables. Each tree is fitted to a random subset of the available variables in the model. The final classification is made across all of these trees. A more detailed non-technical introduction is here, and the original work is from Leo Breiman (2001). 

To illustrate these ideas and exemplar case will be presented, using fictitious data based on people leaving IBM, and in our substantive case, data collected from UK marriage registrars, at some time before a proposed marriage. Each random forest produces a large number of CAR trees, like those previously shown, developed from a random subset of the available variables. The final prediction comes from an average of these trees.

```{r One RandomForest}
# number of features
n_features <- length(names(Attrition)) - 2 # Training and Attrition

set.seed(5)
# train a default random forest model on the Training data
attr_rf <- ranger(
  Attrition ~ ., 
  data = TRAIN,
  num.trees = 1000,
  mtry = floor(n_features / 3),
  probability=TRUE,
  importance = 'impurity',
  respect.unordered.factors = "order",
  min.bucket = 10
)
attr_rf

# get OOB RMSE
default_rmse <- sqrt(attr_rf$prediction.error)
## [1] 0.33

#treeInfo(attr_rf)
importance(attr_rf)
```

```{r Predict from random forest}
predict_ranger_train <- predict(attr_rf, TRAIN)
predict_ranger_test <- predict(attr_rf, TEST)

predictions_train <- predictions(predict_ranger_train)
predictions_test <- predictions(predict_ranger_test)

RF_data_train = bind_cols(Attrition = as.numeric(TRAIN$Attrition) - 1,
                    Prediction = predictions_train[,2])

RF_data_test = bind_cols(Attrition = as.numeric(TEST$Attrition) - 1,
                    Prediction = predictions_test[,2])
```

Fitting the random forest model, produces a set of predictions, and one summary of these prediction is shown in the graph for training and test data. The predictions are better than those of the CAR tree model for both data sets, but there is a considerable gap between the outcome for the training data, on which the model was developed, and the test data, which is real data which was not used to train the model.

```{r Plots from random forest}
gTree_ranger_train <- ggplot(RF_data_train, aes(d = Attrition,
                         m = Prediction)) +
    geom_roc(colour = 'blue', labelsize = 3) +
  geom_abline(color='red', slope = 1, intercept = 0)


gTree_ranger_test <- ggplot(RF_data_test, aes(d = Attrition,
                         m = Prediction)) +
    geom_roc(colour = 'blue', labelsize = 3) +
  geom_abline(color='red', slope = 1, intercept = 0)

AUC_ranger_train <- calc_auc(gTree_ranger_train)
AUC_ranger_test <- calc_auc(gTree_ranger_test)


gTree_ranger_train <- gTree_ranger_train +
    labs(title = 'Random forest - training data',
         subtitle = paste('Area under curve = ', round(AUC_ranger_train$AUC, 2)),
         x = "False positives",
         y = "True positives",
    )

gTree_ranger_test <- gTree_ranger_test +
    labs(title = 'Random forest - Test data',
         subtitle = paste('Area under curve = ', round(AUC_ranger_test$AUC, 2)),
         x = "False positives",
         y = "True positives",
    )
cowplot::plot_grid(nrow = 2, gTree_ranger_train, gTree_ranger_test)
```

In much the same way as for a single classification tree, the importance of the variables in a random forest can be calculated.

```{r Importance plot for a ranger}

Variable <- names(importance(attr_rf))
Importance <- as.numeric(importance(attr_rf))
DATA = bind_cols(Variable = Variable, Importance = Importance)

ggplot(data = DATA %>% arrange(Importance) %>% filter(Importance > 5),
       aes(y = forcats::fct_inorder(Variable), x = Importance)) +
  labs(title = 'Variable importance from a Random Forest',
       subtitle = 'Importance over 5',
         x  = 'Variable importance (model based measure)',
         y = 'Variable') +
  geom_col(colour = 'black', fill = 'lightblue') +
  theme_minimal() +
  theme(text = element_text(size = 25))

```

Note that the relative impact of the variables is rather different from the case of the single tree fitted above. In particular all the salary related variables are more important, and paid overtime is much less important, in the random forest model, than in the classification tree.

## Performance on Training and Test data

There are a wide range of measures proposed to measure the performance of machine learning models. These cover a range of objectives, and there is no single measure of performance that is optimal in all circumstances. For purposes of illustration, it is useful to compare a range of measures between the training data, and the test data.

```{r Performance on Training and Test data}
Performance_training <- RF_data_train %>%
  mutate(Attrition = as_factor(Attrition)) %>%
  mutate(Prediction = as_factor(
    case_when(Prediction < 0.15 ~ 0,
              Prediction >= 0.15 ~ 1))) %>%
  conf_mat(truth=Attrition, estimate = Prediction) %>%
  summary()

names(Performance_training) <- c('Measure', 'Estimator', 'Training')


Performance_test <- RF_data_test %>%
  mutate(Attrition = as_factor(Attrition)) %>%
  mutate(Prediction = as_factor(
    case_when(Prediction < 0.15 ~ 0,
              Prediction >= 0.15 ~ 1))) %>%
  conf_mat(truth = Attrition, estimate = Prediction) %>%
  summary()

names(Performance_test) <- c('Measure', 'Estimator', 'Test')

Performance <- full_join(Performance_training, Performance_test,
                         by = join_by(Measure, Estimator)) %>%
  select(-Estimator) %>%
  pivot_longer(!Measure, names_to = 'Type', values_to = 'Test') %>%
  mutate(Measure = case_match(Measure,
                              'accuracy' ~ 'Accuracy',
                              'kap' ~ 'Kappa',
                              'sens' ~ 'Sensitivity',
                              'spec' ~ 'Specificity',
                              'ppv' ~ 'Positive pv',
                              'npv' ~ 'Negative pv',
                              'mcc' ~ 'Matthews cc',
                              'j_index' ~ 'J_index',
                              'bal_accuracy' ~ 'Balanced accuracy',
                              'detection_prevalence' ~ 'Detection prevalence',
                              'precision' ~ 'Precision',
                              'recall' ~ 'Recall',
                              'f_meas' ~ 'F_measure'
                              )) %>%
  mutate(Measure = fct_inorder(as_factor(Measure))) %>%
  mutate(Type = fct_inorder(as_factor(Type)))
```

This graph shows, for training and test data, a suite of indicators of performance. In different applications, some of these measures might be more or less important, however the finding that the model performs significantly worse on test data than on training data, is of importance.

```{r Graph performance on training and test data}
ggplot(data = Performance,
       aes(x = Measure, y = Test, group = Type, fill = Type)) +
  geom_col(position = 'dodge') +
  labs(title = 'Performance metrics',
       subtitle = 'Comparing traing and test data'
  ) +
  scale_fill_brewer(palette = 'Paired') +
  guides(x = guide_axis(angle = 45)) +
  theme_minimal()
```

```{r Tidy up random forest}
rm(predictions_test, predictions_train, predict_ranger_test, predict_ranger_train)
rm(AUC_ranger_test, AUC_ranger_train)
rm(gTree_ranger_test, gTree_ranger_train)
rm (attr_rf)
rm(n_features, default_rmse, Importance, Variable)
rm(DATA, Performance, Performance_test, Performance_training)
```

# Impacts of random error and bias

The data in our example, whether or not someone left the company, can be taken to be very reliable. To match more closely with our substantive example, the Home Office sham marriage analysis tool, consider using these models to predict the risk of leaving. The impact of this, in practice, is hard to know, as a manager might conceivably either give more attention to an employee perceived to be at risk of leaving, or might exclude that person from opportunities for development, depending on their views of the appropriate response. By contrast, there may be little benefit to a couple from being investigated as a potential sham marriage, and referred for further investigation, unless evidence of coercion is identified.

Two possible effects might be seen given this situation. The assessed risk of leaving might be quite inaccurate, or it might be biassed, perhaps because of the prior belief of a manager that certain categories of employee are more likely to leave than others. In the sham marriage context, this would be seen as referral for investigation based on either errors by the registrars in deciding which cases to refer, or prejudice by the registrars in referring certain cases more than others. In the first situation, either too many or too few, couples would be referred but this excess would be fairly distributed across all couples presenting; in the second situation, certain couples would be at higher (or lower) risk of referral, because the registrars had a prior belief that certain irrelevant characteristics were more likely to be associated with sham marriage.

To explore these effects further, a set of simulation studies are required. The first set examines the effect of adding various degrees of random error to the data, and the second examines the effect of bias. Of course, in this contrived situation, we know the true values - the values of Attrition from the original dataset, so there is a gold standard available for comparison.

## Random error

For the first analysis the aim is to illustrate the impact of random error in the training data on the performance of a random forest (RF) classifier. For this, the true values of leaving (or staying) are changed randomly. The question is what happens to the classifier performance, measured as the area under the ROC curve, as this error rate rises. There are three case, first how the classifier performs on the randomly altered data, second how it performs on the original training data, and third how it performs on the original test data.

```{r swap_binary function}
################################################
# Switches a random proportion of 1/0 values to 0/1
#  
swap_binary <- function(A, prob = 0.1) {
  N    <- length(A)
  size <- 1 # Each trial is a single attempt

  # Draw  1 random 0/1 per value in A
  RANDOM <- rbinom(N,size,prob)

  A <- bind_cols(A = A, RANDOM  = RANDOM)
  #If the random value is 1 swap the values, in A, otherwise leave them alone
  A <- A %>% 
    mutate(RESULT  = case_when(
                        (A == 0 & RANDOM == 0) ~ 0,
                        (A == 0 & RANDOM == 1) ~ 1,
                        (A == 1 & RANDOM == 0) ~ 1,
                        (A == 1 & RANDOM == 1) ~ 0,
                        .default = NA
                        ))
 
return(A$RESULT)
}


A <- c(0,0,1,1,0,0,1,1,0,0,0,1,0,1,0,1,0,1,1,1,0,0,0)
XX <- swap_binary(A, prob=0.0)  
XX <- swap_binary(A, prob=0.5)  
XX <- swap_binary(A, prob=1.0)  

#############################################
# Swaps Yes and No in a factor
# 
swap_YesNo <- function(A, prob = 0.1) {
  N    <- length(A)
  size <- 1 # Each trial is a single attempt

  # Draw  1 random 0/1 per value in A
  RANDOM <- rbinom(N,size,prob)

for (i in seq_along(A)) {
  if (A[i] == 'No' && RANDOM[i] == 0) {
      A[i] = 'No'
  } else
    if (A[i] == 'No' && RANDOM[i] == 1) {
      A[i] = 'Yes'
  } else
    if (A[i] == 'Yes' && RANDOM[i] == 0) {
      A[i] = 'Yes'
  } else
    if (A[i] == 'Yes' && RANDOM[i] == 1) {
      A[i] = 'No'
    }
}
  
return(A)
}


A <- Attrition$Attrition[1:30]

XX <- swap_YesNo(A, prob=0.0)
XX <- swap_YesNo(A, prob=0.5)  
XX <- swap_YesNo(A, prob=1.0)  

rm(A, XX)
```

```{r Calculate auc manually}
# data is a dataframe with truth
#
AUC_calc <- function(data, d, m) {
  d <- data[[d]]
    if (is.factor(d)) { d <- as.integer(d) - 1}
  m <- data[[m]]

  DATA <- bind_cols(Truth = d, Prediction = m)
#    cat('str(DATA',str(DATA),'\n')
  
  DATA <- DATA %>%
    arrange(desc(Prediction)) %>%
    mutate(TPR = cumsum(Truth)/sum(Truth)) %>%
    mutate(FPR = cumsum(!Truth)/sum(!Truth))

    
  AUC <- DescTools::AUC(x = DATA$FPR, y = DATA$TPR, method='step', ties = 'mean')
    
#  return(round(AUC,4))
return( AUC)
}

AUC_calc(RF_data_train, d = 'Attrition', m = 'Prediction')
AUC_calc(RF_data_test, d = 'Attrition', m = 'Prediction')

rm(RF_data_test, RF_data_train)
```

```{r Setup for Random Error}
set.seed(5)

# Add noise to Attrition
Working <- Attrition %>%
  mutate(NoisyAttrition =
           swap_YesNo(Attrition, prob = 0.1))
TRAIN <- Working %>% filter(Training)
TEST  <- Working %>% filter(!Training)
  table(TRAIN$Attrition, TRAIN$NoisyAttrition)
  table(TEST$Attrition, TEST$NoisyAttrition)

```

```{r Random forest check}

# number of features
n_features <- length(names(Attrition)) - 2 # Training and Attrition

# train a default random forest model on the Training data
# We have to remove the original Attrition, and the Training variables
attr_rf1 <- ranger(
  NoisyAttrition ~ ., 
  data = TRAIN %>% select(-Attrition, -Training), # Remove selected variables
  num.trees = 1000,
  mtry = floor(n_features / 3),
  probability = TRUE,
  importance = 'impurity',
  respect.unordered.factors = "order",
  min.bucket = 10
)
attr_rf1

# get OOB RMSE
default_rmse <- sqrt(attr_rf1$prediction.error)
## [1] 0.33

#treeInfo(attr_rf1)
importance(attr_rf1)

```

```{r Predict from random forest 2}
predict_ranger_train <- predict(attr_rf1, TRAIN)
predict_ranger_test <- predict(attr_rf1, TEST)

predictions_train <- predictions(predict_ranger_train)
predictions_test <- predictions(predict_ranger_test)

RF_data_train = bind_cols(Attrition = TRAIN$NoisyAttrition,
                    Prediction = predictions_train[,2])

RF_data_test = bind_cols(Attrition = TEST$Attrition,
                    Prediction = predictions_test[,2])
```

```{r Plots from random forest 2}
gTree_ranger_train <- ggplot(RF_data_train, aes(d = Attrition,
                         m = Prediction)) +
    geom_roc(colour = 'blue', labelsize = 3) +
  geom_abline(color='red', slope = 1, intercept = 0)


gTree_ranger_test <- ggplot(RF_data_test, aes(d = Attrition,
                         m = Prediction)) +
    geom_roc(colour = 'blue', labelsize = 3) +
  geom_abline(color='red', slope = 1, intercept = 0)

AUC_ranger_train <- calc_auc(gTree_ranger_train)
AUC_calc(RF_data_train, d = 'Attrition', m = 'Prediction')

AUC_ranger_test <- calc_auc(gTree_ranger_test)
AUC_calc(RF_data_test, d = 'Attrition', m = 'Prediction')


gTree_ranger_train <- gTree_ranger_train +
    labs(title = 'Random forest - Training data (noisy)',
         subtitle = paste('Area under curve = ', round(AUC_ranger_train$AUC, 2)),
         x = "False positives",
         y = "True positives",
    )

gTree_ranger_test <- gTree_ranger_test +
    labs(title = 'Random forest - Test data (noisy)',
         subtitle = paste('Area under curve = ', round(AUC_ranger_test$AUC, 2)),
         x = "False positives",
         y = "True positives",
    )

cowplot::plot_grid(nrow = 2, gTree_ranger_train, gTree_ranger_test)
```

```{r Function for noisy simulation}

OneModel <- function(Data, prob = 0.0){
  # number of features
  n_features <- length(names(Data)) - 2 # Training and Attrition

  probability_of_swapping <- prob
  
  Working <- Data %>%
  mutate(NoisyAttrition =
           swap_YesNo(Attrition, prob = probability_of_swapping))

  TRAIN <- Working %>% filter(Training)
  TEST  <- Working %>% filter(!Training)

#  cat('TABLE \n',table(TRAIN$Attrition, TRAIN$NoisyAttrition),'\n')
#  cat('TABLE \n',table(TEST$Attrition, TEST$NoisyAttrition),'\n')

# train a default random forest model on the Training data
attr_rf1 <- ranger(
  NoisyAttrition ~ ., 
  data = TRAIN %>% select(-Attrition, -Training),
  num.trees = 1000,
  mtry = floor(n_features / 3),
  probability=TRUE,
  importance = 'impurity',
  respect.unordered.factors = "order",
  min.bucket = 10
)

  predict_ranger_train <- predict(attr_rf1, TRAIN)
  predict_ranger_test <- predict(attr_rf1, TEST)

  predictions_train <- predictions(predict_ranger_train)
  predictions_test <- predictions(predict_ranger_test)

  RF_data_train_1 = bind_cols(Attrition = TRAIN$Attrition,
                    Prediction = predictions_train[,2])

  RF_data_train_2 = bind_cols(NoisyAttrition = TRAIN$NoisyAttrition,
                    Prediction = predictions_train[,2])

  RF_data_test_1 = bind_cols(Attrition = TEST$Attrition,
                    Prediction = predictions_test[,2])
  
  RF_data_test_2 = bind_cols(NoisyAttrition = TEST$NoisyAttrition,
                    Prediction = predictions_test[,2])

 # plot(pROC::roc(RF_data$NoisyAttrition, RF_data$Prediction))
  Area_under_Curve_train_1 <- AUC_calc(RF_data_train_1,
                             d = 'Attrition',
                             m = 'Prediction')
  Area_under_Curve_train_2 <- AUC_calc(RF_data_train_2,
                             d = 'NoisyAttrition',
                             m = 'Prediction')
  Area_under_Curve_test_1   <- AUC_calc(RF_data_test_1,
                             d = 'Attrition',
                             m = 'Prediction')
  Area_under_Curve_test_2   <- AUC_calc(RF_data_test_2,
                             d = 'NoisyAttrition',
                             m = 'Prediction')
#cat(Area_under_Curve,"\n")
return(c(AUC_Train_Attrition = Area_under_Curve_train_1,
            AUC_Train_Noise = Area_under_Curve_train_2, 
            AUC_Test_Attrition  = Area_under_Curve_test_1,
            AUC_Test_Noise  = Area_under_Curve_test_2
         ))
}

OneModel(Data = Attrition, prob = 0.0)
#AUC_Train_Attrition     AUC_Train_Noise  AUC_Test_Attrition      AUC_Test_Noise 
#          0.9505894           0.9505894           0.8108145           0.8108145 
# 
OneModel(Data = Attrition, prob = 0.1)
#AUC_Train_Attrition     AUC_Train_Noise  AUC_Test_Attrition      AUC_Test_Noise 
#          0.9274953           0.9374958           0.7825512           0.7087735  

OneModel(Data = Attrition, prob =  0.2)
# AUC_Train_Attrition     AUC_Train_Noise  AUC_Test_Attrition      AUC_Test_Noise 
#          0.8411537           0.9215150           0.7183118           0.5611185

```

```{r Run the noisy simulations (SLOW)}
ModelNoisySims <- seq(0,0.15, 0.01) %>%
     set_names() %>%
     map(~replicate(1000, OneModel(Data = Attrition, prob = .x) ) )

readr::write_rds(ModelNoisySims, file = "data/ModelNoisySims.Rds")
#ModelNoisySims <- readr::read_rds(file = "data/ModelNoisySims.Rds")

```

```{r Post-process the noisy simulations}
D <- do.call(rbind, lapply(ModelNoisySims, data.frame)) %>%
  rownames_to_column('NAME') %>%
  mutate(Error_rate = str_split(NAME,"\\.[AUC_]", simplify = TRUE )[,1]) %>%
  mutate(Set = str_extract(NAME,"\\_(.*?)\\_")) %>%
  mutate(Set = str_replace_all(Set, "\\_", "" )) %>%
  mutate(Type = str_extract(NAME,"[^_]+$")) %>%
  select(NAME,Error_rate,Set,Type,X1:X1000)

D <- D %>%
  pivot_longer(cols=starts_with('X'),
               names_prefix = 'X',
               names_transform = as.integer,
               names_to = 'Index',
               values_to = 'AUC') %>%
  mutate(Error_pct = forcats::fct_inorder(paste(round(as.numeric(Error_rate)*100,0), '%'))) %>%
arrange(Set, Type, Error_rate, Index)
```

```{r Graph the random error simulations}

ggplot(data <- D,
       aes(x = AUC, y = Error_pct, group = interaction(Set,Type,Error_rate),
           colour = Set, fill = Type)) +
    geom_boxplot(outliers = TRUE, outlier.size = 0.8, position = 'dodge2') +
    labs(title = 'Impact of increasing level of error on training and test datasets',
         x = "Area under Curve",
         y = "Simulated error rate in training data") +
    theme_minimal() +
    coord_cartesian(xlim = c(0.5,1)) +
    scale_y_discrete(limits = rev)

```

In this graph each boxplot represents one set of 1,000 runs of an RF classifier. These are run with data for the training data set with various levels of random error added, ranging from none to 15%.

The AUC is calculated for each RF for each of three sets of data - data with added random error (Noisy), which was used to fit the RF's; the original training data, which for these analyses represents the 'true' values of the training data; and the original test data.

What this shows is that for even modest error rates, the estimated performance on noisy data is a gross over-estimate of the performance on clean test data; and the performance of the classifier on noisy data is much worse, and both deteriorates, and becomes much more variable (represented by the width of each boxplot)  sharply as the level of noise increases.

# Bias

Random error can also be described as noise. It is a source of variation in the data which is independent of the variables in the data. Bias is a source of systematic error. In our toy example, perhaps managers falsely believe that workers in a particular category were more likely to leave than others.

In our substantive example, if registrars were more likely to refer people for investigation who had certain characteristics, but which were not in fact associated with sham marriage, this could give rise to systematic injustice and a range of harms. This is acknowledged by the Home Office AQA review which lists as one of the model assumption "Historical decisions (which determine target variable labels for model training) are accurate and free from bias." (AAQ page 3, Section 4). The variable used in the Home Office model have been redacted from their report, but by implication from the text, the opinion of the registrar as to the interaction between the couple, and the age difference between the couple are among those considered (EIA page 6 and 7).

In order to model bias, it is necessary to choose sources for the bias. For illustrative purposes only, the variable EnvironmentSatisfaction can be used. This has some significance, having an importance of around 7 in the models considered here, but is not a dominant predictor for Attrition. To illustrate the impact of bias we shift the values of attrition according to the value of EnvironmentSatisfaction. This shift is random, but is driven by a single value - the scale of bias, which serves a similar function in this modelling exercise, to the degree of noise in the previous section.

```{r bias_binary function}

Scale_to_0_1 <- function(Variable) {
  stopifnot( is.numeric( Variable ))

ScaledVariable <- {(Variable-min(Variable))/(max(Variable)-min(Variable))}

  return(ScaledVariable)
}

# Check
s <- sort(rexp(100))
range(s)
s <- Scale_to_0_1(s)
range(s)

################################################
# Switches a proportion of 1/0 values to 0/1
#  depending on the value of another variable, and a parameter called Level_of_bias
#  
bias_YesNo <- function(Variable, Source_of_bias, Level_of_bias = 0.0) {
  N   <- length(Variable)

# Check to see if there is one value of Source_of_bias for every value of Variable
  M   <- length(Source_of_bias)
      stopifnot(M == N)
      stopifnot(is.numeric(Source_of_bias))

# Scale Source_of_bias to 0...1
  Source_of_bias_scaled <- Scale_to_0_1(Source_of_bias)
#  cat('Source_of_bias_scaled',Source_of_bias_scaled,'\n')

    size <- 1 # Each trial is a single attempt
    prob <- pmin(Source_of_bias_scaled * Level_of_bias, 1) # Limit to 1
# Draw  1 random 0/1 per value in A
    RANDOM <- rbinom(N,size,prob)
#    cat(' N, size, prob', N, size, prob, '\n')
#    cat('RANDOM ', RANDOM, '\n')
  A <- bind_cols(Variable = Variable, RANDOM  = RANDOM)
#  cat('str (A)', str(A),'\n')
  for (i in seq_along(A$Variable)) {
    if (A$Variable[i] == 'No' && A$RANDOM[i] == 0) {
        A$Variable[i] = 'No'
    } else
      if (A$Variable[i] == 'No' && A$RANDOM[i] == 1) {
        A$Variable[i] = 'Yes'
    } else
      if (A$Variable[i] == 'Yes' && A$RANDOM[i] == 0) {
        A$Variable[i] = 'Yes'
    } else
      if (A$Variable[i] == 'Yes' && A$RANDOM[i] == 1) {
        A$Variable[i] = 'Yes' # Introduces an asymmetry
      }
}

return(A$Variable)
}

# Check
TEST <- rbinom(400,1,0.2)
Variable <- as_factor(TEST) %>% fct_inorder() %>% fct_recode(Yes = '1', No = '0')
Source <-   rep(c(0,1,2,3,4),80) 
table(Variable,Source)

DATA <- bind_cols(Variable = Variable, Source = Source) %>%
  mutate(Bias_0.0 = bias_YesNo(Variable, Source, Level_of_bias = 0.0)) %>%
  mutate(Bias_0.05 = bias_YesNo(Variable, Source, Level_of_bias = 0.05)) %>%
  mutate(Bias_0.1 = bias_YesNo(Variable, Source, Level_of_bias = 0.1)) %>%
  mutate(Bias_0.15 = bias_YesNo(Variable, Source, Level_of_bias = 0.15)) %>%
  mutate(Bias_0.2 = bias_YesNo(Variable, Source, Level_of_bias = 0.2)) %>%
  mutate(Bias_1.0 = bias_YesNo(Variable, Source, Level_of_bias = 1.0))
  
table(DATA$Variable, DATA$Bias_0.0, DATA$Source)
table(DATA$Variable, DATA$Bias_0.05, DATA$Source)
table(DATA$Variable, DATA$Bias_0.1, DATA$Source)
table(DATA$Variable, DATA$Bias_0.15, DATA$Source)
table(DATA$Variable, DATA$Bias_0.2, DATA$Source)
table(DATA$Variable, DATA$Bias_1.0, DATA$Source)

rm(DATA)
```

```{r Setup for Bias}
set.seed(465465)

# Add bias to Attrition
Working <- Attrition %>%
  mutate(BiassedAttrition =
           bias_YesNo(Variable = Attrition,
                       Source_of_bias = as.numeric(EnvironmentSatisfaction),
                       Level_of_bias = 0.0))
  table(Working$Attrition, Working$BiassedAttrition)

# Make test and training datasets
TRAIN <- Working %>% filter(Training)
TEST  <- Working %>% filter(!Training)
```

```{r Random forest bias check}
set.seed(5)
# train a default random forest model on the Training data
# We have to remove the original Attrition, and the Training variables
attr_rf1 <- ranger(
  BiassedAttrition ~ ., 
  data = TRAIN %>% select(-Attrition, -Training), # Remove selected variables
  num.trees = 1000,
  mtry = floor(n_features / 3),
  probability = TRUE,
  importance = 'impurity',
  respect.unordered.factors = "order",
  min.bucket = 10
)
attr_rf1

# get OOB RMSE
default_rmse <- sqrt(attr_rf1$prediction.error)
## [1] 0.33

#treeInfo(attr_rf1)
importance(attr_rf1)

```

```{r Predict from random forest 3}
predict_ranger_train <- predict(attr_rf1, TRAIN)
predict_ranger_test <- predict(attr_rf1, TEST)

predictions_train <- predictions(predict_ranger_train)
predictions_test <- predictions(predict_ranger_test)

RF_data_train = bind_cols(Attrition = TRAIN$BiassedAttrition,
                    Prediction = predictions_train[,2])

RF_data_test = bind_cols(Attrition = TEST$BiassedAttrition,
                    Prediction = predictions_test[,2])
```

```{r Plots from random forest 3}
gTree_ranger_train <- ggplot(RF_data_train, aes(d = Attrition,
                         m = Prediction)) +
    geom_roc(colour = 'blue', labelsize = 3) +
  geom_abline(color='red', slope = 1, intercept = 0)


gTree_ranger_test <- ggplot(RF_data_test, aes(d = Attrition,
                         m = Prediction)) +
    geom_roc(colour = 'blue', labelsize = 3) +
  geom_abline(color='red', slope = 1, intercept = 0)

AUC_ranger_train <- calc_auc(gTree_ranger_train)
AUC_calc(RF_data_train, d = 'Attrition', m = 'Prediction')

AUC_ranger_test <- calc_auc(gTree_ranger_test)
AUC_calc(RF_data_test, d = 'Attrition', m = 'Prediction')


gTree_ranger_train <- gTree_ranger_train +
    labs(title = 'Random forest - Training data (biassed)',
         subtitle = paste('Area under curve = ', round(AUC_ranger_train$AUC, 2)),
         x = "False positives",
         y = "True positives",
    )

gTree_ranger_test <- gTree_ranger_test +
    labs(title = 'Random forest - Test data (biassed)',
         subtitle = paste('Area under curve = ', round(AUC_ranger_test$AUC, 2)),
         x = "False positives",
         y = "True positives",
    )

cowplot::plot_grid(nrow = 2, gTree_ranger_train, gTree_ranger_test)
```

```{r Function for biassed simulation}

OneBiassedModel <- function(Data, Source_of_bias, Level_of_bias = 0.1){
  # number of features
  n_features <- length(names(Data)) - 2 # Training and Attrition

  Working <- Data %>%
    mutate(BiassedAttrition =
           bias_YesNo(Attrition,
                      Source_of_bias = as.numeric(EnvironmentSatisfaction),
                      Level_of_bias = Level_of_bias))

  TRAIN <- Working %>% filter(Training)
  TEST  <- Working %>% filter(!Training)

  cat('TABLE \n',table(TRAIN$Attrition, TRAIN$BiassedAttrition),'\n')
  cat('TABLE \n',table(TEST$Attrition, TEST$BiassedAttrition),'\n')


# train a default random forest model on the Training data
attr_rf1 <- ranger(
  BiassedAttrition ~ ., 
  data = TRAIN %>% select(-Attrition, -Training),
  num.trees = 1000,
  mtry = floor(n_features / 3),
  probability=TRUE,
  importance = 'impurity',
  respect.unordered.factors = "order",
  min.bucket = 10
)
  predict_ranger_train <- predict(attr_rf1, TRAIN)
  predict_ranger_test <- predict(attr_rf1, TEST)

  predictions_train <- predictions(predict_ranger_train)
  predictions_test <- predictions(predict_ranger_test)

  RF_data_train_1 = bind_cols(Attrition = TRAIN$Attrition,
                    Prediction = predictions_train[,2])

  RF_data_train_2 = bind_cols(BiassedAttrition = TRAIN$BiassedAttrition,
                    Prediction = predictions_train[,2])

  RF_data_test_1 = bind_cols(Attrition = TEST$Attrition,
                    Prediction = predictions_test[,2])
  
  RF_data_test_2 = bind_cols(BiassedAttrition = TEST$BiassedAttrition,
                    Prediction = predictions_test[,2])

 # plot(pROC::roc(RF_data$BiassedAttrition, RF_data$Prediction))
  Area_under_Curve_train_1 <- AUC_calc(RF_data_train_1,
                             d = 'Attrition',
                             m = 'Prediction')
  Area_under_Curve_train_2 <- AUC_calc(RF_data_train_2,
                             d = 'BiassedAttrition',
                             m = 'Prediction')
  Area_under_Curve_test_1   <- AUC_calc(RF_data_test_1,
                             d = 'Attrition',
                             m = 'Prediction')
  Area_under_Curve_test_2   <- AUC_calc(RF_data_test_2,
                             d = 'BiassedAttrition',
                             m = 'Prediction')
#cat(Area_under_Curve,"\n")
return(c(AUC_Train_Attrition = Area_under_Curve_train_1,
            AUC_Train_Bias = Area_under_Curve_train_2, 
            AUC_Test_Attrition  = Area_under_Curve_test_1,
            AUC_Test_Bias  = Area_under_Curve_test_2
         ))
}

OneBiassedModel(Data = Attrition,
         Source_of_bias = EnvironmentSatisfaction,
         Level_of_bias = 0.0)
# AUC_Train_Attrition     AUC_Train_Noise  AUC_Test_Attrition       AUC_Test_Bias 
# 0.9512131               0.9512131        0.8051510                0.8051510 
          

OneBiassedModel(Data = Attrition,
         Source_of_bias = EnvironmentSatisfaction,
         Level_of_bias = 0.1)
#AUC_Train_Attrition     AUC_Train_Noise  AUC_Test_Attrition       AUC_Test_Bias 
#          0.9387764           0.9309477           0.8013215           0.7457398 
OneBiassedModel(Data = Attrition,
         Source_of_bias = EnvironmentSatisfaction,
         Level_of_bias = 0.2)
#AUC_Train_Attrition     AUC_Train_Noise  AUC_Test_Attrition       AUC_Test_Bias 
#          0.9333101           0.9365522           0.8024002           0.6653573 

```

```{r Run the biassed simulations (SLOW)}
ModelBiassedSims <- seq(0,0.15, 0.01) %>%
     set_names() %>%
     map(~replicate(1000, OneBiassedModel(Data = Attrition,
                                          Source_of_bias = EnvironmentSatisfaction,
                                          Level_of_bias = .x) ) )
#
readr::write_rds(ModelBiassedSims, file = "data/ModelBiassedSims.Rds")
#ModelBiassedSims <- readr::read_rds(file = "data/ModelBiassedSims.Rds")

```

```{r Post-process the biassed simulations}
D <- do.call(rbind, lapply(ModelBiassedSims, data.frame)) %>%
  rownames_to_column('NAME') %>%
  mutate(Error_rate = str_split(NAME,"\\.[AUC_]", simplify = TRUE )[,1]) %>%
  mutate(Set = str_extract(NAME,"\\_(.*?)\\_")) %>%
  mutate(Set = str_replace_all(Set, "\\_", "" )) %>%
  mutate(Type = str_extract(NAME,"[^_]+$")) %>%
  select(NAME,Error_rate,Set,Type,X1:X1000)

D <- D %>%
  pivot_longer(cols=starts_with('X'),
               names_prefix = 'X',
               names_transform = as.integer,
               names_to = 'Index',
               values_to = 'AUC') %>%
  mutate(Error_pct = forcats::fct_inorder(paste(round(as.numeric(Error_rate)*100,0), '%'))) %>%
  arrange(Set, Type, Error_rate, Index)
```

```{r Graph the biassed simulations}

ggplot(data <- D,
       aes(x = AUC, y = Error_pct, group = interaction(Set,Type,Error_rate),
           colour = Set, fill = Type)) +
    geom_boxplot(outliers = TRUE, outlier.size = 0.8, position = 'dodge2') +
    labs(title = 'Impact of increasing level of bias on training and test datasets',
         x = "Area under Curve",
         y = "Simulated bias rate in training data") +
    theme_minimal() +
    coord_cartesian(xlim = c(0.5,1)) +
    scale_y_discrete(limits = rev)

```
